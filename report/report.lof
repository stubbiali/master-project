\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite {Kri07}.\relax }}{6}{figure.caption.8}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace {} y_{s_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace {} y_{s_m} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , s_m$, and fires $y_j$, sent to the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. The neuron threshold $\theta _j$ is reported within its body.\relax }}{9}{figure.caption.9}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace {} y_{s_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace {} y_{s_m}, \tmspace +\thinmuskip {.1667em} -\theta _j {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , s_m, \tmspace +\thinmuskip {.1667em} b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. Observe that, conversely to the model offered in Fig. \ref {fig:neural-model}, the neuron threshold is now set to $0$.\relax }}{10}{figure.caption.10}
\contentsline {figure}{\numberline {1.4}{\ignorespaces Left: logistic function \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:logistic}\unskip \@@italiccorr )}} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resembles the Heaviside function. Right: hyperbolic tangent \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:hyperbolic-tangent}\unskip \@@italiccorr )}}.\relax }}{11}{figure.caption.11}
\contentsline {figure}{\numberline {1.5}{\ignorespaces A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }}{13}{figure.caption.12}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Representation of the boundary conditions for the Laplace problems \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:laplace-problem-weight-function}\unskip \@@italiccorr )}} (\emph {left}) and \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:laplace-problem-projection-function}\unskip \@@italiccorr )}} (\emph {right}) stated on a exagonal reference domain $\Omega $.\relax }}{40}{figure.caption.17}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }}{41}{figure.caption.18}
\contentsline {figure}{\numberline {2.3}{\ignorespaces The parametrized computational domain (solid line) for the Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:nonlinear-poisson-example}\unskip \@@italiccorr )}}.\relax }}{62}{figure.caption.19}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: average relative error between the FE solution for problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:nonlinear-poisson-example}\unskip \@@italiccorr )}} and either its projection onto the reduced space (red squares) or the POD-Galerkin RB solution (blue circles); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal {P}$ consisting of $N_{te} = 50$ randomly picked values. Right: comparison between the online run times for the FE scheme and the POD-Galerkin method on $\Xi _{te}$, with $L = 35$ basis functions included in the reduced model.\relax }}{62}{figure.caption.20}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Three point sets randomly generate through the latin hypercube sampling. Observe the good coverage provided by the union of the sets, with only a few overlapping points.\relax }}{65}{figure.caption.21}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: error analysis for the POD-G RB method applied to \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-1}\unskip \@@italiccorr )}}; the results refer to a POD basis constructed based on $N = 25$, $50$, $100$ snapshots; for $N = 100$, the singular values of the corresponding snapshot matrix are shown as well. Right: FE and POD-G solutions for three parameter values.\relax }}{71}{figure.caption.22}
\contentsline {figure}{\numberline {3.2}{\ignorespaces In respect to the approximation of the map \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:map-to-approximate}\unskip \@@italiccorr )}} for the Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-1}\unskip \@@italiccorr )}} via neural networks, comparison between three supervised algorithms - Levenberg-Marquardt (blue circles), Scaled Conjugate Gradient (orange squares), BFGS Quasi-Newton (yellow traingles) - in terms of the optimal configuration detected (\emph {left}) and the associated (averaged) test error (\emph {right}) for different amounts of training samples.\relax }}{72}{figure.caption.23}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Left: relative error yielded by the POD-NN RB method applied to \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-1}\unskip \@@italiccorr )}}; the convergence to the projection error for $L = 10$ (solid line) is analyzed in terms of both the number of neurons included in the neural network and the dimension of the training set. Right: FE and POD-NN solutions for three parameter values.\relax }}{72}{figure.caption.23}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Error analysis for the POD-Galerkin RB method applied to the problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-2}\unskip \@@italiccorr )}}. The lower-bound provided by the projection error \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:projection-error}\unskip \@@italiccorr )}} is reported as reference.\relax }}{73}{figure.caption.24}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Left: error analysis for the POD-NN RB method (dotted lines) applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-2}\unskip \@@italiccorr )}}. The solid solid sections refer to the steps carried out by the routine \ref {alg:podnn-training}; the error yielded by the POD-G method using $L = 8$ or $L = 15$ basis functions are reported as reference. Right: comparison between FE and POD-NN solutions for three input vectors.\relax }}{74}{figure.caption.25}
\contentsline {figure}{\numberline {3.6}{\ignorespaces From left to right, top to bottom: regression plots for the first, seventh, eighth and nine scalar components of the outputs provided on $\Xi _{te}$ by a network with $H_1 = H_2 = 25$ neurons per hidden layer, trained to approximate the map \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:map-to-approximate}\unskip \@@italiccorr )}} for problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-2}\unskip \@@italiccorr )}}.\relax }}{75}{figure.caption.26}
\contentsline {figure}{\numberline {3.7}{\ignorespaces The first $10$ POD basis functions for the (nonlinear) Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-3}\unskip \@@italiccorr )}}.\relax }}{76}{figure.caption.27}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\psi _1$}}}{76}{subfigure.7.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\psi _2$}}}{76}{subfigure.7.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {$\psi _3$}}}{76}{subfigure.7.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {$\psi _4$}}}{76}{subfigure.7.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {$\psi _5$}}}{76}{subfigure.7.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {$\psi _6$}}}{76}{subfigure.7.6}
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {$\psi _7$}}}{76}{subfigure.7.7}
\contentsline {subfigure}{\numberline {(h)}{\ignorespaces {$\psi _8$}}}{76}{subfigure.7.8}
\contentsline {subfigure}{\numberline {(i)}{\ignorespaces {$\psi _9$}}}{76}{subfigure.7.9}
\contentsline {subfigure}{\numberline {(j)}{\ignorespaces {$\psi _{10}$}}}{76}{subfigure.7.10}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Convergence analysis for the POD-G and POD-NN methods applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-3}\unskip \@@italiccorr )}} (\emph {left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph {right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer, and considering $L = 10$ POD modes.\relax }}{76}{figure.caption.28}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Sensitivity analysis for the POD-NN method applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson1d-3}\unskip \@@italiccorr )}} with respect to the number of POD coefficients (per sample) used during the training; to prevent overfitting, $N_{tr} = 300$ learning patterns have been used.\relax }}{77}{figure.caption.29}
\contentsline {figure}{\numberline {3.10}{\ignorespaces The quadrilateral domain used in the simulations (\emph {left}, solid line) and the parametrizations of its sides (\emph {right}).\relax }}{78}{figure.caption.30}
\contentsline {figure}{\numberline {3.11}{\ignorespaces The stenosis geometry employed in the simulations (\emph {left}) and the parametrizations of its sides (\emph {right}).\relax }}{78}{figure.caption.30}
\contentsline {figure}{\numberline {3.12}{\ignorespaces Online run times for the FE, POD-G and POD-NN methods applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-1}\unskip \@@italiccorr )}}, for $N_{te} = 100$ randomly generated parameter values. \relax }}{80}{figure.caption.31}
\contentsline {figure}{\numberline {3.13}{\ignorespaces On the right, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-1}\unskip \@@italiccorr )}}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the left. The solid tracts denote the steps actually pursued by the automatic training routine \ref {alg:podnn-training}. All the results refer to neural networks exposed to $L = 15$ generalized coordinates per learning sample.\relax }}{80}{figure.caption.32}
\contentsline {figure}{\numberline {3.14}{\ignorespaces Finite element (\emph {left}), POD-G (\emph {center}) and POD-NN (\emph {right}) solution to the semilinear Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-2}\unskip \@@italiccorr )}} re-stated over the reference domain, with $\bm {\mu } = (0.835, \tmspace +\thinmuskip {.1667em} 0.034)$.\relax }}{82}{figure.caption.33}
\contentsline {figure}{\numberline {3.15}{\ignorespaces Online relative errors (\emph {left}) and run times (\emph {right}) for the POD-G and the POD-NN methods applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-2}\unskip \@@italiccorr )}} for $N_{te} = 50$ randomly picked parameter values.\relax }}{82}{figure.caption.33}
\contentsline {figure}{\numberline {3.16}{\ignorespaces Convergence analysis for the POD-NN RB method applied to the BVP \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-2}\unskip \@@italiccorr )}}. The results have been obtained via three-layers perceptrons with $H_1 = H_2 \in {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }15, \tmspace +\thinmuskip {.1667em} 20, \tmspace +\thinmuskip {.1667em} 25, \tmspace +\thinmuskip {.1667em} 30, \tmspace +\thinmuskip {.1667em} 35 {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ neurons per hidden layer, and trained with $N_{tr} \in {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }100, \tmspace +\thinmuskip {.1667em} 200, \tmspace +\thinmuskip {.1667em} 300 {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ learning patterns. Each pattern consists of an input vector $\bm {\mu } \in \mathcal {P}$ and $L = 35$ POD coefficients ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } \mathbf {u}_h(\bm {\mu }), \bm {\psi }_i {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }_{\math@bb {R}^M}$, $i = 1, \tmspace +\thinmuskip {.1667em} \ldots \tmspace +\thinmuskip {.1667em} , L$, as teaching inputs.\relax }}{82}{figure.caption.33}
\contentsline {figure}{\numberline {3.17}{\ignorespaces FE solution (\emph {top left}) to the Poisson problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-3}\unskip \@@italiccorr )}} with $\bm {\mu } = (0.349, \tmspace +\thinmuskip {.1667em} -0.413, \tmspace +\thinmuskip {.1667em} 4.257)$, and pointwise errors yielded by either its projection onto $V_{\texttt {rb}}$ (\emph {top right}), or the POD-G method (\emph {bottom left}), or the POD-NN method (\emph {bottom right}). The results have been obtained by considering $L = 30$ POD modes.\relax }}{83}{figure.caption.34}
\contentsline {figure}{\numberline {3.18}{\ignorespaces Error analysis (\emph {left}) and online CPU time (\emph {right}) for the POD-G and the POD-NN methods applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-3}\unskip \@@italiccorr )}} for $N_{te} = 50$ randomly picked parameter values. The reduced basis have been generated via POD, relying on $N = 100$ snapshots. The second plot refers to RB models including $L = 30$ modal functions; within the POD-NN framework, a neural network emboding $35$ neurons per inner layer has been used.\relax }}{84}{figure.caption.35}
\contentsline {figure}{\numberline {3.19}{\ignorespaces Convergence analysis with respect to the number of hidden neurons (\emph {left}) and modal functions (\emph {right}) used within the POD-NN framework applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-3}\unskip \@@italiccorr )}}. The results provided in the first plot have been obtained using $L = 30$ modes; the solid tracts refer to the steps performed by Algorithm \ref {alg:podnn-training}.\relax }}{85}{figure.caption.36}
\contentsline {figure}{\numberline {3.20}{\ignorespaces Average relative errors (\emph {left}) and online run times (\emph {right}) on $\Xi _{te}$ for the POD-CS and the POD-NN methods applied to problem \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:poisson2d-3}\unskip \@@italiccorr )}}. For the latter, the results refer to an MLP equipped with $H_1 = H_2 = 35$ neurons per hidden layer.\relax }}{86}{figure.caption.37}
\contentsline {figure}{\numberline {3.21}{\ignorespaces Computational domain (\emph {left}) and enforced velocity at the boundaries (\emph {right}) for the lid-driven cavity problem for the uncompressible Navier-Stokes equations.\relax }}{87}{figure.caption.38}
\contentsline {figure}{\numberline {3.22}{\ignorespaces The computational mesh $\Omega _h$ used in the simulations.\relax }}{87}{figure.caption.39}
\contentsline {figure}{\numberline {3.23}{\ignorespaces Velocity streamlines (\emph {top}) and pressure plot (\emph {bottom}) for the lid-driven cavity problem, computed through the FE method. Three different parameter values are considered; for all configurations, the Reynold's number is $400$.\relax }}{88}{figure.caption.40}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {2 \pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} 1, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{2} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {2 \pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} 1, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{2} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.6}
\contentsline {figure}{\numberline {3.24}{\ignorespaces Velocity streamlines for the lid-driven cavity benchmark with $\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \sqrt {2}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{4} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and either $Re = 200$ (\emph {left}) or $Re = 400$ (\emph {right}). The solutions have been computed via $\math@bb {P}^2$-$\math@bb {P}^1$ finite elements.\relax }}{89}{figure.caption.41}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \sqrt {2}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{4} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$, $Re = 200$}}}{89}{subfigure.24.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \sqrt {2}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{4} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$, $Re = 400$}}}{89}{subfigure.24.2}
\contentsline {figure}{\numberline {3.25}{\ignorespaces First $5$ POD modes for the velocity (\emph {left}), supremizers (\emph {center}) and pressure (\emph {right}) for the parametrized lid-driven cavity problem with $Re = 200$.\relax }}{90}{figure.caption.42}
\contentsline {figure}{\numberline {3.26}{\ignorespaces Velocity (\emph {left}) and pressure (\emph {right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 200$.\relax }}{91}{figure.caption.43}
\contentsline {figure}{\numberline {3.27}{\ignorespaces Velocity (\emph {left}) and pressure (\emph {right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 400$.\relax }}{92}{figure.caption.44}
\contentsline {figure}{\numberline {3.28}{\ignorespaces Convergence analysis with respect to the number of hidden neurons and training samples used within the POD-NN procedure to approximate the velocity field (\emph {left}) and the pressure distribution (\emph {right}) by means of $35$ modal functions. The Reynold's number is either $200$ (\emph {top}) or $400$ (\emph {bottom}).\relax }}{93}{figure.caption.45}
\contentsline {figure}{\numberline {3.29}{\ignorespaces Online run times for the POD-G and the POD-NN method applied to the lid-driven cavity problem with $Re = 200$ (\emph {left}) and $Re = 400$ (\emph {right}). $N_{te} = 75$ test configurations are considered. For the latter method, the reported times include the (sequential) evaluation of both neural networks for the velocity and pressure field.\relax }}{93}{figure.caption.46}
\contentsline {figure}{\numberline {3.30}{\ignorespaces Pressure contour at three parameter values, as computed through the FE (\emph {top row}), POD-G (\emph {middle row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{94}{figure.caption.47}
\contentsline {figure}{\numberline {3.31}{\ignorespaces $\mathaccentV {widetilde}365{x}$-velocity contour at three parameter values, as computed through the FE (\emph {top row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{95}{figure.caption.48}
\contentsline {figure}{\numberline {3.32}{\ignorespaces $\mathaccentV {widetilde}365{y}$-velocity contour at three parameter values, as computed through the FE (\emph {top row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{95}{figure.caption.49}
\contentsline {figure}{\numberline {3.33}{\ignorespaces Streamlines at three parameter values, as computed through the FE (\emph {top row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{96}{figure.caption.50}
